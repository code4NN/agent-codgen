project_root: .

directories:
  agent:
    files:
      agent.py:
        content: |
          class Agent:
              """
              Primary reasoning agent.
              Proposes next action based on screened state.
              """

              def decide(self, state_view):
                  raise NotImplementedError("Agent decision logic not implemented")

  controller:
    files:
      controller.py:
        content: |
          class Controller:
              """
              Enforces iteration bounds, hypothesis uniqueness,
              and action validity. Does NOT reason.
              """

              def __init__(self, max_iterations=3):
                  self.iteration = 0
                  self.used_hypotheses = set()
                  self.max_iterations = max_iterations

              def allow(self, action):
                  if action.hypothesis in self.used_hypotheses:
                      return False, "Repeated hypothesis"
                  if self.iteration >= self.max_iterations:
                      return False, "Iteration limit reached"
                  return True, "Approved"

              def register(self, action):
                  self.used_hypotheses.add(action.hypothesis)
                  self.iteration += 1

      state.py:
        content: |
          from enum import Enum

          class ControllerState(str, Enum):
              INIT = "INIT"
              SPEC_READY = "SPEC_READY"
              TESTS_READY = "TESTS_READY"
              CODE_READY = "CODE_READY"
              EXECUTED = "EXECUTED"
              SUCCESS = "SUCCESS"
              FAIL = "FAIL"

      policies.py:
        content: |
          MAX_ITERATIONS = 3
          MIN_CONFIDENCE = 0.75

  tools:
    files:
      parse_problem.py: {}
      clarify_problem.py: {}
      normalize_spec.py:
        content: |
          def normalize_spec(spec):
              """
              Validates and locks the problem specification.
              """
              if spec.confidence < 0.75:
                  raise ValueError("Low confidence problem spec")

              for amb in spec.ambiguities:
                  if amb.blocking:
                      raise ValueError("Blocking ambiguity unresolved")

              return spec

      screen_state.py:
        content: |
          def screen_state(global_state):
              """
              Produce a reduced, relevant view of state for the agent.
              Does NOT mutate global state.
              """
              return {
                  "controller_state": global_state.get("controller_state"),
                  "locked_spec": global_state.get("problem_spec"),
                  "last_error": global_state.get("last_error"),
                  "failed_hypotheses": list(global_state.get("failed_hypotheses", []))
              }

      generate_tests.py: {}
      generate_code.py: {}
      debug_code.py: {}
      execute_code.py:
        content: |
          def execute_code(code: str, tests: list):
              """
              Executes code against tests.
              Executor is the ONLY oracle.
              """
              try:
                  exec_globals = {}
                  exec(code, exec_globals)
              except Exception as e:
                  return {"passed": False, "error": str(e)}

              fn = list(exec_globals.values())[-1]

              for test in tests:
                  try:
                      output = fn(*test.inputs)
                      if test.assertion_type == "exact":
                          if output != test.expected:
                              return {"passed": False, "error": "Assertion failed"}
                  except Exception as e:
                      return {"passed": False, "error": str(e)}

              return {"passed": True, "error": None}

  contracts:
    files:
      problem_spec.py:
        content: |
          from typing import List, Literal
          from pydantic import BaseModel

          class Argument(BaseModel):
              name: str
              type: str

          class Ambiguity(BaseModel):
              description: str
              options: List[str]
              default: str
              blocking: bool

          class ProblemSpec(BaseModel):
              function_name: str
              inputs: List[Argument]
              output_type: str
              input_mode: Literal["positional", "keyword"]
              multiple_valid_outputs: bool
              edge_cases: List[str]
              ambiguities: List[Ambiguity]
              confidence: float

      action.py:
        content: |
          from typing import Literal
          from pydantic import BaseModel

          class Action(BaseModel):
              intent: Literal[
                  "parse_problem",
                  "clarify_problem",
                  "generate_tests",
                  "generate_code",
                  "execute_code",
                  "debug",
                  "abstain"
              ]
              hypothesis: str
              tool: str
              rationale: str

      test_case.py:
        content: |
          from typing import Any, Literal
          from pydantic import BaseModel

          class TestCase(BaseModel):
              inputs: Any
              expected: Any
              assertion_type: Literal["exact", "set", "predicate"]
              justification: str

  memory:
    files:
      run_memory.py:
        content: |
          class RunMemory:
              """
              Tracks failed hypotheses within a single run.
              """

              def __init__(self):
                  self.failed = set()

              def add(self, hypothesis: str):
                  self.failed.add(hypothesis)

              def seen(self, hypothesis: str) -> bool:
                  return hypothesis in self.failed

  examples:
    directories:
      problems:
        files:
          two_sum.txt: {}
          palindrome.txt: {}
          ambiguous_problem.txt: {}
      specs:
        files:
          two_sum.json: {}
      tests:
        files:
          two_sum_tests.json: {}

  tests:
    files:
      test_parse_problem.py: {}
      test_normalize_spec.py: {}
      test_controller_loop.py: {}
      test_executor.py: {}
  
  llm:
    files:
      base.py:
        content: |
          from abc import ABC, abstractmethod
          from llm.types import LLMRequest, LLMResponse

          class BaseLLMClient(ABC):
              """
              Abstract base class for all LLM clients.
              """

              @abstractmethod
              def generate(self, request: LLMRequest) -> LLMResponse:
                  pass

      types.py:
        content: |
          from typing import Optional, Dict
          from pydantic import BaseModel

          class LLMRequest(BaseModel):
              prompt: str
              system_prompt: Optional[str] = None
              temperature: float = 0.0
              max_tokens: int = 512

          class LLMResponse(BaseModel):
              text: str
              raw: Dict

      client.py:
        content: |
          from llm.registry import get_llm_client
          from llm.types import LLMRequest

          def call_llm(model: str, **kwargs):
              client = get_llm_client(model)
              request = LLMRequest(**kwargs)
              return client.generate(request)

      registry.py:
        content: |
          from llm.openai_client import OpenAIClient
          from llm.gemini_client import GeminiClient

          def get_llm_client(model: str):
              if model.startswith("gpt"):
                  return OpenAIClient(model)
              if model.startswith("gemini"):
                  return GeminiClient(model)
              raise ValueError(f"Unknown model: {model}")

      openai_client.py:
        content: |
          from llm.base import BaseLLMClient
          from llm.types import LLMRequest, LLMResponse

          class OpenAIClient(BaseLLMClient):
              def __init__(self, model: str):
                  self.model = model

              def generate(self, request: LLMRequest) -> LLMResponse:
                  raise NotImplementedError("OpenAI client not wired yet")

      gemini_client.py:
        content: |
          from llm.base import BaseLLMClient
          from llm.types import LLMRequest, LLMResponse

          class GeminiClient(BaseLLMClient):
              def __init__(self, model: str):
                  self.model = model

              def generate(self, request: LLMRequest) -> LLMResponse:
                  raise NotImplementedError("Gemini client not wired yet")

  prompts:
    files:
      _loader.py:
        content: |
          from pathlib import Path
          from typing import Dict, Optional

          PROMPT_DIR = Path(__file__).parent

          def load_prompt(name: str) -> str:
              """
              Load a prompt template by filename (without extension).
              Example: load_prompt("parse_problem")
              """
              path = PROMPT_DIR / f"{name}.md"
              if not path.exists():
                  raise FileNotFoundError(f"Prompt not found: {path}")
              return path.read_text(encoding="utf-8")

          def render_prompt(template: str, variables: Optional[Dict[str, str]] = None) -> str:
              """
              Minimal variable substitution.
              Uses {{var}} placeholders.
              """
              if not variables:
                  return template

              for key, value in variables.items():
                  template = template.replace(f"{{{{{key}}}}}", str(value))

              return template

      parse_problem.md:
        content: |
          SYSTEM:
          You are a tool that extracts a precise, executable problem specification.

          TASK:
          Convert the given programming problem into a structured specification.

          RULES:
          - Do not assume missing details
          - Explicitly list ambiguities
          - Follow the provided schema strictly

      generate_tests.md:
        content: |
          SYSTEM:
          You are a tool that generates test cases acting as execution oracles.

          TASK:
          Generate test cases strictly from the locked problem specification.

          RULES:
          - Respect ambiguity rules
          - Use correct assertion types
          - Do not invent constraints

      generate_code.md:
        content: |
          SYSTEM:
          You are a tool that generates Python code.

          TASK:
          Generate code that strictly follows the given specification and tests.

          RULES:
          - Follow the function signature exactly
          - No extra output
          - No assumptions beyond the spec

      debug_code.md:
        content: |
          SYSTEM:
          You are a tool that proposes a single fix hypothesis.

          TASK:
          Based on execution failure, suggest one concrete fix or abstain.

          RULES:
          - One hypothesis only
          - No speculative fixes
          - Abstain if no valid fix exists

files:
  main.py:
    content: |
      def main():
          print("CodeGen v2 initialized")

      if __name__ == "__main__":
          main()

  README.md:
    content: |
      # CodeGen v2

      Generic agentic framework.

      - Agent proposes actions
      - Controller enforces discipline
      - Tools execute reality
